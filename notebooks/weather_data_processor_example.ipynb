{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weather Data Processing Example\n",
        "\n",
        "This notebook demonstrates how to use the `WeatherDataProcessor` module to replicate the data processing workflow from the original experiment notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the WeatherDataProcessor from the local module\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
        "\n",
        "from brayam_pineda_ml import WeatherDataProcessor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Weather Data Processor\n",
        "\n",
        "The `WeatherDataProcessor` is initialized with Sydney coordinates and timezone, matching the original experiment setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WeatherDataProcessor initialized for Sydney, Australia\n",
            "Latitude: -33.8678\n",
            "Longitude: 151.2073\n",
            "Timezone: Australia/Sydney\n"
          ]
        }
      ],
      "source": [
        "# Initialize the weather data processor with Sydney coordinates\n",
        "processor = WeatherDataProcessor(\n",
        "    lat=-33.8678,  # Sydney latitude\n",
        "    lon=151.2073,  # Sydney longitude\n",
        "    timezone=\"Australia/Sydney\"\n",
        ")\n",
        "\n",
        "print(\"WeatherDataProcessor initialized for Sydney, Australia\")\n",
        "print(f\"Latitude: {processor.lat}\")\n",
        "print(f\"Longitude: {processor.lon}\")\n",
        "print(f\"Timezone: {processor.timezone}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Complete Pipeline Processing\n",
        "\n",
        "The `process_full_pipeline` method replicates the entire data processing workflow from the original notebook in a single call.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Run the complete pipeline for classification (rain prediction)\n",
        "# print(\"Running complete data processing pipeline...\")\n",
        "# print(\"This includes: fetching data, creating targets, feature engineering, splitting, scaling, and imputation\")\n",
        "\n",
        "# data = processor.process_full_pipeline(\n",
        "#     start_date=\"2016-01-01\",\n",
        "#     end_date=\"2024-12-31\",\n",
        "#     task_type=\"classification\",\n",
        "#     target_name=\"target_rain\"\n",
        "# )\n",
        "\n",
        "# print(\"\\n✅ Data processing complete!\")\n",
        "# print(f\"Training set: {data['X_train'].shape}\")\n",
        "# print(f\"Validation set: {data['X_val'].shape}\")\n",
        "# print(f\"Test set: {data['X_test'].shape}\")\n",
        "# print(f\"Number of features: {len(data['feature_names'])}\")\n",
        "# print(f\"Target variable: {data['target_name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Explore the Processed Data\n",
        "\n",
        "Let's examine the processed data to understand what features were created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Display basic information about the processed data\n",
        "# print(\"=== TRAINING DATA INFO ===\")\n",
        "# print(f\"Shape: {data['X_train'].shape}\")\n",
        "# print(f\"Features: {list(data['X_train'].columns)[:10]}...\")  # Show first 10 features\n",
        "# print(f\"Target distribution: {data['y_train'].value_counts().to_dict()}\")\n",
        "\n",
        "# print(\"\\n=== FEATURE TYPES ===\")\n",
        "# feature_types = data['X_train'].dtypes.value_counts()\n",
        "# print(feature_types)\n",
        "\n",
        "# print(\"\\n=== MISSING VALUES ===\")\n",
        "# missing_values = data['X_train'].isnull().sum()\n",
        "# print(f\"Features with missing values: {(missing_values > 0).sum()}\")\n",
        "# if (missing_values > 0).any():\n",
        "#     print(missing_values[missing_values > 0].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Step-by-Step Processing Example\n",
        "\n",
        "Now let's demonstrate how to use individual methods for more granular control over the processing pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-30 17:45:12.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrayam_pineda_ml.weather_data_processor\u001b[0m:\u001b[36mfetch_weather_data\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mFetching weather data from 2016-01-01 to 2024-12-31\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 1: FETCHING WEATHER DATA ===\n"
          ]
        },
        {
          "ename": "HTTPError",
          "evalue": "429 Client Error: Too Many Requests for url: https://archive-api.open-meteo.com/v1/archive?latitude=-33.8678&longitude=151.2073&start_date=2016-01-01&end_date=2024-12-31&daily=weather_code,temperature_2m_max,temperature_2m_min,apparent_temperature_max,apparent_temperature_min,precipitation_sum,rain_sum,snowfall_sum,precipitation_hours,wind_speed_10m_max,wind_gusts_10m_max,wind_direction_10m_dominant,shortwave_radiation_sum,et0_fao_evapotranspiration,sunshine_duration,daylight_duration&timezone=Australia/Sydney",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 1: Fetch raw weather data\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== STEP 1: FETCHING WEATHER DATA ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m raw_data = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_weather_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2016-01-01\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2024-12-31\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRaw data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_data.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(raw_data.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - UTS\\Master\\Spring 2025\\AML\\adv_mla_2025\\aml_kaggle_chll\\src\\brayam_pineda_ml\\weather_data_processor.py:66\u001b[39m, in \u001b[36mWeatherDataProcessor.fetch_weather_data\u001b[39m\u001b[34m(self, start_date, end_date)\u001b[39m\n\u001b[32m     64\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching weather data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m response = requests.get(url)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m data = response.json()\n\u001b[32m     69\u001b[39m df = pd.DataFrame(data[\u001b[33m\"\u001b[39m\u001b[33mdaily\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brayam Pineda\\anaconda3\\envs\\aml-assignment-2-clean\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 429 Client Error: Too Many Requests for url: https://archive-api.open-meteo.com/v1/archive?latitude=-33.8678&longitude=151.2073&start_date=2016-01-01&end_date=2024-12-31&daily=weather_code,temperature_2m_max,temperature_2m_min,apparent_temperature_max,apparent_temperature_min,precipitation_sum,rain_sum,snowfall_sum,precipitation_hours,wind_speed_10m_max,wind_gusts_10m_max,wind_direction_10m_dominant,shortwave_radiation_sum,et0_fao_evapotranspiration,sunshine_duration,daylight_duration&timezone=Australia/Sydney"
          ]
        }
      ],
      "source": [
        "# Step 1: Fetch raw weather data\n",
        "print(\"=== STEP 1: FETCHING WEATHER DATA ===\")\n",
        "raw_data = processor.fetch_weather_data(\"2016-01-01\", \"2024-12-31\")\n",
        "print(f\"Raw data shape: {raw_data.shape}\")\n",
        "print(f\"Columns: {list(raw_data.columns)}\")\n",
        "print(f\"Date range: {raw_data['time'].min()} to {raw_data['time'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-30 17:36:00.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrayam_pineda_ml.weather_data_processor\u001b[0m:\u001b[36mcreate_classification_target\u001b[0m:\u001b[36m128\u001b[0m - \u001b[1mCreated classification target 'target_rain' with 3281 samples\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 2: CREATING CLASSIFICATION TARGET ===\n",
            "Data with target shape: (3281, 19)\n",
            "Target distribution: {1: 1682, 0: 1599}\n",
            "Target percentage: 51.26%\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Create classification target\n",
        "print(\"=== STEP 2: CREATING CLASSIFICATION TARGET ===\")\n",
        "data_with_target = processor.create_classification_target(\n",
        "    raw_data, \n",
        "    target_name=\"target_rain\", \n",
        "    threshold=0.1, \n",
        "    horizon_days=7\n",
        ")\n",
        "print(f\"Data with target shape: {data_with_target.shape}\")\n",
        "print(f\"Target distribution: {data_with_target['target_rain'].value_counts().to_dict()}\")\n",
        "print(f\"Target percentage: {data_with_target['target_rain'].mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-30 17:36:00.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrayam_pineda_ml.weather_data_processor\u001b[0m:\u001b[36madd_temporal_features\u001b[0m:\u001b[36m174\u001b[0m - \u001b[1mAdded temporal features and filtered to keep: ['et0_fao_evapotranspiration', 'precipitation_sum', 'wind_speed_10m_max', 'year', 'shortwave_radiation_sum', 'season', 'wind_gusts_10m_max', 'sunshine_duration', 'weather_code', 'wind_direction_10m_dominant', 'temperature_2m_max', 'time', 'apparent_temperature_max', 'apparent_temperature_min', 'rain_sum', 'month', 'temperature_2m_min', 'target_rain']\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 3: ADDING TEMPORAL FEATURES ===\n",
            "Added temporal features: year, month, season\n",
            "Season distribution: {'Autumn': 828, 'Winter': 828, 'Spring': 819, 'Summer': 806}\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Add temporal features\n",
        "\n",
        "features_keep = [\n",
        "    \"temperature_2m_max\", \"temperature_2m_min\",\n",
        "    \"apparent_temperature_max\", \"apparent_temperature_min\",\n",
        "    \"wind_speed_10m_max\", \"wind_gusts_10m_max\",\n",
        "    \"shortwave_radiation_sum\", \"sunshine_duration\", \"et0_fao_evapotranspiration\",\n",
        "    \"weather_code\", \"wind_direction_10m_dominant\", \n",
        "    \"time\", \"precipitation_sum\", \"rain_sum\", \"target_rain\"\n",
        "]\n",
        "\n",
        "print(\"=== STEP 3: ADDING TEMPORAL FEATURES ===\")\n",
        "data_with_temporal = processor.add_temporal_features(data_with_target, keep_columns=features_keep)\n",
        "print(f\"Added temporal features: year, month, season\")\n",
        "print(f\"Season distribution: {data_with_temporal['season'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3281 entries, 0 to 3280\n",
            "Data columns (total 18 columns):\n",
            " #   Column                       Non-Null Count  Dtype         \n",
            "---  ------                       --------------  -----         \n",
            " 0   et0_fao_evapotranspiration   3281 non-null   float64       \n",
            " 1   precipitation_sum            3281 non-null   float64       \n",
            " 2   wind_speed_10m_max           3281 non-null   float64       \n",
            " 3   year                         3281 non-null   int32         \n",
            " 4   shortwave_radiation_sum      3281 non-null   float64       \n",
            " 5   season                       3281 non-null   category      \n",
            " 6   wind_gusts_10m_max           3281 non-null   float64       \n",
            " 7   sunshine_duration            3281 non-null   float64       \n",
            " 8   weather_code                 3281 non-null   int64         \n",
            " 9   wind_direction_10m_dominant  3281 non-null   int64         \n",
            " 10  temperature_2m_max           3281 non-null   float64       \n",
            " 11  time                         3281 non-null   datetime64[ns]\n",
            " 12  apparent_temperature_max     3281 non-null   float64       \n",
            " 13  apparent_temperature_min     3281 non-null   float64       \n",
            " 14  rain_sum                     3281 non-null   float64       \n",
            " 15  month                        3281 non-null   int32         \n",
            " 16  temperature_2m_min           3281 non-null   float64       \n",
            " 17  target_rain                  3281 non-null   int64         \n",
            "dtypes: category(1), datetime64[ns](1), float64(11), int32(2), int64(3)\n",
            "memory usage: 413.7 KB\n"
          ]
        }
      ],
      "source": [
        "data_with_temporal.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-30 17:35:28.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrayam_pineda_ml.weather_data_processor\u001b[0m:\u001b[36mcreate_lag_features\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mCreated lag features for rain_sum: [1, 2, 3, 7]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 4: CREATING LAG FEATURES ===\n",
            "Added lag features for rain_sum: [1, 2, 3, 7] days\n",
            "New lag columns: ['lag1', 'lag2', 'lag3', 'lag7']\n"
          ]
        }
      ],
      "source": [
        "# # Step 4: Create lag features\n",
        "# print(\"=== STEP 4: CREATING LAG FEATURES ===\")\n",
        "# data_with_lags = processor.create_lag_features(\n",
        "#     data_with_temporal, \n",
        "#     target_col=\"rain_sum\", \n",
        "#     lags=[1, 2, 3, 7]\n",
        "# )\n",
        "# print(f\"Added lag features for rain_sum: [1, 2, 3, 7] days\")\n",
        "# print(f\"New lag columns: {[col for col in data_with_lags.columns if 'lag' in col]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-30 17:36:11.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrayam_pineda_ml.weather_data_processor\u001b[0m:\u001b[36mcreate_rolling_features\u001b[0m:\u001b[36m228\u001b[0m - \u001b[1mCreated rolling features for windows: [3, 7, 14, 30]\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 5: CREATING ROLLING FEATURES ===\n",
            "Added rolling features for windows: [3, 7, 14, 30] days\n",
            "New rolling columns: ['precip_3d_sum', 'precip_3d_avg', 'precip_3d_std', 'precip_3d_max', 'rain_days_3d', 'precip_7d_sum', 'precip_7d_avg', 'precip_7d_std', 'precip_7d_max', 'rain_days_7d', 'precip_14d_sum', 'precip_14d_avg', 'precip_14d_std', 'precip_14d_max', 'rain_days_14d', 'precip_30d_sum', 'precip_30d_avg', 'precip_30d_std', 'precip_30d_max', 'rain_days_30d']\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Create rolling features\n",
        "print(\"=== STEP 5: CREATING ROLLING FEATURES ===\")\n",
        "data_with_rolling = processor.create_rolling_features(\n",
        "    data_with_temporal, \n",
        "    windows=[3, 7, 14, 30]\n",
        ")\n",
        "print(f\"Added rolling features for windows: [3, 7, 14, 30] days\")\n",
        "print(f\"New rolling columns: {[col for col in data_with_rolling.columns if any(x in col for x in ['3d', '7d', '14d', '30d'])]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Create advanced features\n",
        "print(\"=== STEP 6: CREATING ADVANCED FEATURES ===\")\n",
        "data_with_advanced = processor.create_advanced_features(data_with_rolling)\n",
        "print(f\"Added advanced meteorological features\")\n",
        "print(f\"Total features now: {len(data_with_advanced.columns)}\")\n",
        "print(f\"Advanced feature examples: {[col for col in data_with_advanced.columns if any(x in col for x in ['temp_range', 'storm_potential', 'instability'])]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Encode categorical features\n",
        "print(\"=== STEP 7: ENCODING CATEGORICAL FEATURES ===\")\n",
        "data_encoded = processor.encode_categorical_features(\n",
        "    data_with_advanced, \n",
        "    categorical_cols=[\"season\"]\n",
        ")\n",
        "print(f\"Encoded categorical features: season\")\n",
        "print(f\"Season dummy columns: {[col for col in data_encoded.columns if 'season' in col]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Split data chronologically\n",
        "print(\"=== STEP 8: SPLITTING DATA CHRONOLOGICALLY ===\")\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = processor.split_time_series_data(\n",
        "    data_encoded, \n",
        "    target_col=\"target_rain\",\n",
        "    train_ratio=0.7, \n",
        "    val_ratio=0.15\n",
        ")\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"Target distributions - Train: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Target distributions - Val: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"Target distributions - Test: {y_test.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Impute missing values\n",
        "print(\"=== STEP 9: IMPUTING MISSING VALUES ===\")\n",
        "X_train_imp, X_val_imp, X_test_imp = processor.impute_missing_values(\n",
        "    X_train, X_val, X_test, \n",
        "    strategy=\"mean\"\n",
        ")\n",
        "print(f\"Imputed missing values using mean strategy\")\n",
        "print(f\"Missing values after imputation: {X_train_imp.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Scale features\n",
        "print(\"=== STEP 10: SCALING FEATURES ===\")\n",
        "X_train_scaled, X_val_scaled, X_test_scaled = processor.scale_features(\n",
        "    X_train_imp, X_val_imp, X_test_imp, \n",
        "    method=\"standard\"\n",
        ")\n",
        "print(f\"Scaled features using standard scaling\")\n",
        "print(f\"Scaled data shape: {X_train_scaled.shape}\")\n",
        "print(f\"Feature scaling statistics (first 5 features):\")\n",
        "print(X_train_scaled.iloc[:, :5].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Training and Evaluation\n",
        "\n",
        "Now let's train a simple model to demonstrate the complete workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a simple Random Forest model\n",
        "print(\"=== TRAINING RANDOM FOREST MODEL ===\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train on processed data\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "print(\"Model trained successfully!\")\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = rf_model.predict(X_train_scaled)\n",
        "y_val_pred = rf_model.predict(X_val_scaled)\n",
        "y_test_pred = rf_model.predict(X_test_scaled)\n",
        "\n",
        "y_train_proba = rf_model.predict_proba(X_train_scaled)[:, 1]\n",
        "y_val_proba = rf_model.predict_proba(X_val_scaled)[:, 1]\n",
        "y_test_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model performance\n",
        "print(\"=== MODEL PERFORMANCE EVALUATION ===\")\n",
        "\n",
        "# Training performance\n",
        "train_auc = roc_auc_score(y_train, y_train_proba)\n",
        "train_f1 = f1_score(y_train, y_train_pred)\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "# Validation performance\n",
        "val_auc = roc_auc_score(y_val, y_val_proba)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "# Test performance\n",
        "test_auc = roc_auc_score(y_test, y_test_proba)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Training - AUROC: {train_auc:.4f}, F1: {train_f1:.4f}, Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation - AUROC: {val_auc:.4f}, F1: {val_f1:.4f}, Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test - AUROC: {test_auc:.4f}, F1: {test_f1:.4f}, Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "overfitting_gap = train_auc - val_auc\n",
        "print(f\"\\nOverfitting gap (Train AUC - Val AUC): {overfitting_gap:.4f}\")\n",
        "if overfitting_gap > 0.05:\n",
        "    print(\"⚠️  Warning: Potential overfitting detected!\")\n",
        "else:\n",
        "    print(\"✅ No significant overfitting detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis\n",
        "\n",
        "Let's examine which features are most important for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== TOP 20 MOST IMPORTANT FEATURES ===\")\n",
        "print(feature_importance.head(20))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 15 Most Important Features')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Regression Example\n",
        "\n",
        "Let's also demonstrate how to use the processor for regression tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Regression pipeline for precipitation prediction\n",
        "print(\"=== REGRESSION PIPELINE EXAMPLE ===\")\n",
        "\n",
        "# Create a new processor instance for regression\n",
        "reg_processor = WeatherDataProcessor(\n",
        "    lat=-33.8678,\n",
        "    lon=151.2073,\n",
        "    timezone=\"Australia/Sydney\"\n",
        ")\n",
        "\n",
        "# Run regression pipeline\n",
        "reg_data = reg_processor.process_full_pipeline(\n",
        "    start_date=\"2016-01-01\",\n",
        "    end_date=\"2024-12-31\",\n",
        "    task_type=\"regression\",\n",
        "    target_name=\"precip_3day_next\"\n",
        ")\n",
        "\n",
        "print(f\"Regression data - Train: {reg_data['X_train'].shape}, Val: {reg_data['X_val'].shape}, Test: {reg_data['X_test'].shape}\")\n",
        "print(f\"Target variable: {reg_data['target_name']}\")\n",
        "print(f\"Target statistics: {reg_data['y_train'].describe()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Benefits\n",
        "\n",
        "The `WeatherDataProcessor` module provides several key benefits:\n",
        "\n",
        "### **Complete Pipeline Processing**\n",
        "- Single method call handles the entire workflow\n",
        "- Consistent with the original notebook processing\n",
        "- Reduces code duplication and errors\n",
        "\n",
        "### **Modular Design**\n",
        "- Individual methods for each processing step\n",
        "- Full control over the processing pipeline\n",
        "- Easy to customize and extend\n",
        "\n",
        "### **Built-in Best Practices**\n",
        "- Chronological data splitting for time series\n",
        "- Proper scaling and imputation\n",
        "- Advanced feature engineering\n",
        "- Categorical encoding\n",
        "\n",
        "### **Flexibility**\n",
        "- Supports both classification and regression tasks\n",
        "- Configurable parameters for all methods\n",
        "- Easy to adapt for different locations and time periods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Feature Selection Example\n",
        "\n",
        "Now let's demonstrate the new feature selection capability using the specific variables from your original notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the specific features to keep (from your original notebook)\n",
        "features_keep = [\n",
        "    # Temperature\n",
        "    \"temperature_2m_max\", \"temperature_2m_min\",\n",
        "    \"apparent_temperature_max\", \"apparent_temperature_min\",\n",
        "\n",
        "    # Wind\n",
        "    \"wind_speed_10m_max\", \"wind_gusts_10m_max\",\n",
        "\n",
        "    # Radiation / sunshine / evapotranspiration\n",
        "    \"shortwave_radiation_sum\", \"sunshine_duration\", \"et0_fao_evapotranspiration\",\n",
        "\n",
        "    # Categorical / context\n",
        "    \"weather_code\", \"wind_direction_10m_dominant\", \n",
        "    \"time\", \"precipitation_sum\", \"rain_sum\", \"target_rain\"\n",
        "]\n",
        "\n",
        "print(\"=== FEATURE SELECTION EXAMPLE ===\")\n",
        "print(f\"Features to keep: {features_keep}\")\n",
        "print(f\"Total features to keep: {len(features_keep)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new processor for feature selection example\n",
        "feature_processor = WeatherDataProcessor(\n",
        "    lat=-33.8678,\n",
        "    lon=151.2073,\n",
        "    timezone=\"Australia/Sydney\"\n",
        ")\n",
        "\n",
        "# Run pipeline with feature selection\n",
        "print(\"Running pipeline with feature selection...\")\n",
        "selected_data = feature_processor.process_full_pipeline(\n",
        "    start_date=\"2016-01-01\",\n",
        "    end_date=\"2024-12-31\",\n",
        "    task_type=\"classification\",\n",
        "    target_name=\"target_rain\",\n",
        "    keep_columns=features_keep  # This is the new parameter!\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Feature selection complete!\")\n",
        "print(f\"Selected training set: {selected_data['X_train'].shape}\")\n",
        "print(f\"Selected validation set: {selected_data['X_val'].shape}\")\n",
        "print(f\"Selected test set: {selected_data['X_test'].shape}\")\n",
        "print(f\"Number of selected features: {len(selected_data['feature_names'])}\")\n",
        "print(f\"Selected features: {selected_data['feature_names']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with the full pipeline (without feature selection)\n",
        "print(\"=== COMPARISON: WITH vs WITHOUT FEATURE SELECTION ===\")\n",
        "print(f\"Full pipeline features: {len(data['feature_names'])}\")\n",
        "print(f\"Selected features: {len(selected_data['feature_names'])}\")\n",
        "print(f\"Reduction: {len(data['feature_names']) - len(selected_data['feature_names'])} features removed\")\n",
        "\n",
        "# Show the difference in feature sets\n",
        "full_features = set(data['feature_names'])\n",
        "selected_features = set(selected_data['feature_names'])\n",
        "removed_features = full_features - selected_features\n",
        "\n",
        "print(f\"\\nRemoved features ({len(removed_features)}):\")\n",
        "for feature in sorted(removed_features):\n",
        "    print(f\"  - {feature}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using add_temporal_features with feature selection\n",
        "print(\"=== INDIVIDUAL METHOD WITH FEATURE SELECTION ===\")\n",
        "\n",
        "# Fetch raw data\n",
        "raw_data = feature_processor.fetch_weather_data(\"2016-01-01\", \"2024-12-31\")\n",
        "\n",
        "# Create target\n",
        "data_with_target = feature_processor.create_classification_target(\n",
        "    raw_data, \n",
        "    target_name=\"target_rain\", \n",
        "    threshold=0.1, \n",
        "    horizon_days=7\n",
        ")\n",
        "\n",
        "# Add temporal features AND filter to keep only specified columns\n",
        "data_filtered = feature_processor.add_temporal_features(\n",
        "    data_with_target, \n",
        "    keep_columns=features_keep\n",
        ")\n",
        "\n",
        "print(f\"Original data shape: {data_with_target.shape}\")\n",
        "print(f\"Filtered data shape: {data_filtered.shape}\")\n",
        "print(f\"Columns kept: {list(data_filtered.columns)}\")\n",
        "print(f\"Features removed: {data_with_target.shape[1] - data_filtered.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== UPDATED PROCESSING SUMMARY ===\")\n",
        "print(f\"✅ Successfully processed weather data from 2016-2024\")\n",
        "print(f\"✅ Created {len(data['feature_names'])} features through advanced engineering\")\n",
        "print(f\"✅ NEW: Feature selection capability added to add_temporal_features()\")\n",
        "print(f\"✅ NEW: Can filter to keep only specific columns during processing\")\n",
        "print(f\"✅ Split data chronologically: {data['X_train'].shape[0]} train, {data['X_val'].shape[0]} val, {data['X_test'].shape[0]} test\")\n",
        "print(f\"✅ Applied scaling, imputation, and encoding\")\n",
        "print(f\"✅ Trained model with AUROC: {test_auc:.4f} on test set\")\n",
        "print(f\"\\nThe WeatherDataProcessor module now includes feature selection capabilities!\")\n",
        "print(f\"This allows you to replicate your original notebook's feature selection approach.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== PROCESSING SUMMARY ===\")\n",
        "print(f\"✅ Successfully processed weather data from 2016-2024\")\n",
        "print(f\"✅ Created {len(data['feature_names'])} features through advanced engineering\")\n",
        "print(f\"✅ Split data chronologically: {data['X_train'].shape[0]} train, {data['X_val'].shape[0]} val, {data['X_test'].shape[0]} test\")\n",
        "print(f\"✅ Applied scaling, imputation, and encoding\")\n",
        "print(f\"✅ Trained model with AUROC: {test_auc:.4f} on test set\")\n",
        "print(f\"\\nThe WeatherDataProcessor module successfully replicates the original notebook workflow!\")\n",
        "print(f\"This demonstrates how the custom package streamlines the data processing pipeline.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aml-assignment-2-clean",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
