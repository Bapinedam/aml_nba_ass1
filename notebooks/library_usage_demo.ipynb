{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Library Usage Demo - Replacing Notebook Code with Library Functions\n",
        "\n",
        "This notebook demonstrates how to replace the manual data processing code in your experiment notebooks with the new library functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before: Manual Data Processing (from your original notebooks)\n",
        "\n",
        "Your original notebooks had code like this:\n",
        "\n",
        "```python\n",
        "# Manual API call\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "LAT, LON = -33.8678, 151.2073\n",
        "TZ = \"Australia/Sydney\"\n",
        "START, END = \"2016-01-01\", \"2024-12-31\"\n",
        "\n",
        "DAILY_VARS = [\n",
        "    \"weather_code\", \"temperature_2m_max\", \"temperature_2m_min\",\n",
        "    \"precipitation_sum\", \"rain_sum\", \"wind_speed_10m_max\", ...\n",
        "]\n",
        "\n",
        "url = f\"https://archive-api.open-meteo.com/v1/archive?latitude={LAT}&longitude={LON}&start_date={START}&end_date={END}&daily={','.join(DAILY_VARS)}&timezone={TZ}\"\n",
        "\n",
        "response = requests.get(url)\n",
        "data_rain = pd.DataFrame(response.json()[\"daily\"])\n",
        "data_rain[\"time\"] = pd.to_datetime(data_rain[\"time\"]).dt.date\n",
        "\n",
        "# Manual target creation\n",
        "data_rain[\"precip_3day_next\"] = (\n",
        "    data_rain[\"precipitation_sum\"].shift(-1) + \n",
        "    data_rain[\"precipitation_sum\"].shift(-2) + \n",
        "    data_rain[\"precipitation_sum\"].shift(-3)\n",
        ")\n",
        "\n",
        "# Manual feature engineering\n",
        "data_rain[\"month\"] = data_rain[\"time\"].dt.month\n",
        "data_rain[\"year\"] = data_rain[\"time\"].dt.year\n",
        "# ... many more manual steps\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## After: Using the Library (Clean and Simple)\n",
        "\n",
        "Now you can replace all that manual code with just a few lines:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the local development version\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('../src'))\n",
        "\n",
        "# Import directly from the module to avoid conflicts with installed package\n",
        "from brayam_pineda_ml.weather_data_processor import WeatherDataProcessor\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize processor\n",
        "processor = WeatherDataProcessor(\n",
        "    lat=-33.8678,\n",
        "    lon=151.2073,\n",
        "    timezone=\"Australia/Sydney\"\n",
        ")\n",
        "\n",
        "# Run complete pipeline for regression\n",
        "data = processor.process_full_pipeline(\n",
        "    start_date=\"2016-01-01\",\n",
        "    end_date=\"2024-12-31\",\n",
        "    task_type=\"regression\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Data processing complete!\")\n",
        "print(f\"Training set: {data['X_train'].shape}\")\n",
        "print(f\"Features: {len(data['feature_names'])}\")\n",
        "print(f\"Target: {data['target_name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Processing (Alternative Approach)\n",
        "\n",
        "You can also run the pipeline step by step for more control:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Fetch data\n",
        "df = processor.fetch_weather_data(\"2016-01-01\", \"2024-12-31\")\n",
        "print(f\"Raw data shape: {df.shape}\")\n",
        "\n",
        "# Step 2: Create target\n",
        "df = processor.create_regression_target(df, \"precip_3day_next\")\n",
        "print(f\"After target creation: {df.shape}\")\n",
        "\n",
        "# Step 3: Add temporal features\n",
        "df = processor.add_temporal_features(df)\n",
        "print(f\"After temporal features: {df.shape}\")\n",
        "\n",
        "# Step 4: Create lag features\n",
        "df = processor.create_lag_features(df, \"precip_3day_next\", [1, 2])\n",
        "print(f\"After lag features: {df.shape}\")\n",
        "\n",
        "# Step 5: Create rolling features\n",
        "df = processor.create_rolling_features(df, [3, 7, 14, 30])\n",
        "print(f\"After rolling features: {df.shape}\")\n",
        "\n",
        "# Step 6: Create advanced features\n",
        "df = processor.create_advanced_features(df)\n",
        "print(f\"After advanced features: {df.shape}\")\n",
        "\n",
        "# Step 7: Encode categorical features\n",
        "df = processor.encode_categorical_features(df, [\"weather_code\", \"season\"])\n",
        "print(f\"After encoding: {df.shape}\")\n",
        "\n",
        "# Step 8: Split data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = processor.split_time_series_data(df, \"precip_3day_next\")\n",
        "print(f\"Split complete - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# Step 9: Impute missing values\n",
        "X_train, X_val, X_test = processor.impute_missing_values(X_train, X_val, X_test)\n",
        "print(\"Missing values imputed\")\n",
        "\n",
        "# Step 10: Scale features\n",
        "X_train, X_val, X_test = processor.scale_features(X_train, X_val, X_test)\n",
        "print(\"Features scaled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For classification tasks, use the classification target\n",
        "df_class = processor.fetch_weather_data(\"2016-01-01\", \"2024-12-31\")\n",
        "df_class = processor.create_classification_target(df_class, \"target_rain\", threshold=0.1, horizon_days=7)\n",
        "\n",
        "# Run the rest of the pipeline\n",
        "data_class = processor.process_full_pipeline(\n",
        "    start_date=\"2016-01-01\",\n",
        "    end_date=\"2024-12-31\",\n",
        "    task_type=\"classification\"\n",
        ")\n",
        "\n",
        "print(f\"Classification data ready!\")\n",
        "print(f\"Training set: {data_class['X_train'].shape}\")\n",
        "print(f\"Class distribution: {data_class['y_train'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benefits of Using the Library\n",
        "\n",
        "1. **Code Reduction**: Replace 100+ lines of manual processing with 5-10 lines\n",
        "2. **Consistency**: Same processing logic across all experiments\n",
        "3. **Maintainability**: Changes to processing logic only need to be made in one place\n",
        "4. **Reusability**: Easy to use the same processing for new experiments\n",
        "5. **Documentation**: Well-documented functions with clear parameters\n",
        "6. **Error Handling**: Built-in validation and error checking\n",
        "7. **Flexibility**: Can use full pipeline or individual steps as needed\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
